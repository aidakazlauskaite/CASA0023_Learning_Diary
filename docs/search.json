[
  {
    "objectID": "Week_7.html#summary",
    "href": "Week_7.html#summary",
    "title": "7  Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis week’s content continues the topic of classification in remote sensing by covering the topics of object-based image analysis, sub-pixel analysis, accuracy assessment and cross-validation.\nBefore we dive in, it’s important to note that there’s data that comes pre-classified, such as the following:\n\nGlobeLand30 - 30m for 2000, 2010 and 2020;\nEuropean Space Agency’s (ESA) Climate Change Initiative (CCI) annual global land cover (300 m) (1992-2015);\nDynamic World - near real time 10m;\nMODIS;\nGoggle building data.\n\n\n7.1.1 Object-based image analysis (OBIA)\nRather than examining individual cells, we analyse shapes formed by grouping together cells that exhibit similar characteristics (homogeneity) or differences (heterogeneity) i.e., superpixels. The most common method to approach this is SLIC (Simple Linear Iterative Clustering) algorithm for superpixel generation by clustering pixels based on color similarity and spatial proximity; it measures compactness (i.e., balance between physical distance and colour) and classifies objects based on their average value.\n\n\n7.1.2 Sub-pixel analysis (also called sub-pixel classification, spectral mixture analysis (SMA), linear spectral unmixing)\nSub-pixel analysis addresses the question of whether it should be classified singularly or should the proportions be computed when dealing with a pixel containing multiple land cover types. This classifier calculates the proportion or abundance of land cover within each pixel.\nConsiderations to keep in mind:\n\nPixel purity;\nNumber of end members;\nMultiple endmember spectral analysis (MESMA).\n\n\n\n7.1.3 Accuracy assessment\nAs a common practice in machine learning, we need to calculate accuracy of our classifier’s outputs.\nAccuracy assessments we are concerned with in remote sensing will be derived from the confusion matrix:\n\n\n\nSource: (Bajaj 2023).\n\n\nProducer’s accuracy is defined as the ratio of correctly classified pixels (TP) to the total ground truth data (TP + FN):\n\\[PA= \\frac{TP}{TP+FN}\\]\nUser’s accuracy is defined as the proportion of correctly classified pixels (TP) out of all pixels classified as a specific land cover (TP + FP):\n\\[UA= \\frac{TP}{TP+FP}\\]\nFinally, the overall accuracy represents the collective proportion of correctly classified pixels (TP + TN) across all land cover types (TP + FP + FN + TN):\n\\[OA= \\frac{TP+TN}{TP+FP+FN+TN}\\]\nNote: there isn’t a sole correct option for accuracy measurements.\nKappa measures the agreement between classifications and labeled data, assessing accuracy beyond chance results.\nThe F1-Score (F Measure) integrates both recall (PA) and Precision (UA):\n\\[F1= \\frac{2 * UA * PA}{UA + PA}\\]\nwhich also equals\n\\[F1= \\frac{TP}{TP+1/2*(FP+FN)}\\]\nApproaches to take in order to make sure our accuracy assessment is truthful:\n\nTrain and test split (roughly 70% train - 30% test data split).\nCross validation - or take it to the extremes and opt for leave one out cross validation (only for smaller datasets).\nSpatial cross validation - to address the issue of potential spatial autocorrelation - partition the folded data spatially, with folds used for cross-validation. If not addressed, the accuracy assessment will claim that classifier performs more precisely than it would do on unseen data in reality."
  },
  {
    "objectID": "Week_7.html#applications",
    "href": "Week_7.html#applications",
    "title": "7  Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nThe application section will review literature where the two new classifier types discussed this week (OBIA and sub-pixel analysis) have been utilised.\nA study by Park et al. (2020) aimed to enhance maritime safety through the detection and monitoring of vessels using airborne hyperspectral imaging. Employing pixel-based mixture techniques and objective ellipse fitting methods, researchers accurately detected and estimated the sizes of marine vessels, particularly fishing boats and yachts. Through various spectral analyses and dimensionality reduction techniques like principal component analysis, the study successfully detected vessels using algorithms like N-FINDR, PPI, ICA, and VCA. The application of these techniques resulted in high probabilities of vessel detection, with a probability of detection (POD) of 96.40% and a false alarm ratio (FAR) of 4.30%, aiding in preventing maritime accidents by providing efficient monitoring over wide coastal areas at high resolutions.\nAnother study by Blaschke, Feizizadeh, and Holbling (2014) had a primary objective to develop a semi-automated object-based image analysis (OBIA) methodology for landslide detection and delineation. By integrating various satellite imagery features such as NDVI, brightness, and textural features with DEM derivatives and GLCMs, landslides were accurately identified within a study area in north-western Iran. The methodology, validated against a landslide inventory database, achieved a high overall accuracy of 93.07%, demonstrating the effectiveness of OBIA in incorporating heterogeneous parameters for landslide classification. This study highlights the potential of GEOBIA as a valuable tool in remote sensing and geographic information science, offering improved accuracy and efficiency in natural disaster prevention and management efforts.\n\n\n\nAreas classified as “potentially affected by landslides” (highlighted in red) superimposed on the IRS panchromatic satellite image (Blaschke, Feizizadeh, and Holbling 2014)."
  },
  {
    "objectID": "Week_7.html#reflections",
    "href": "Week_7.html#reflections",
    "title": "7  Classification II",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\n\n\n\n\nBajaj, Varun, ed. 2023. Artificial Intelligence-Based Brain-Computer Interface. London ; San Diego, CA: Academic Press, An imprint of Elsevier.\n\n\nBlaschke, Thomas, Bakhtiar Feizizadeh, and Daniel Holbling. 2014. “Object-Based Image Analysis and Digital Terrain Analysis for Locating Landslides in the Urmia Lake Basin, Iran.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 7 (12): 4806–17. https://doi.org/10.1109/JSTARS.2014.2350036.\n\n\nPark, Jae-Jin, Tae-Sung Kim, Kyung-Ae Park, Sangwoo Oh, Moonjin Lee, and Pierre-Yves Foucher. 2020. “Application of Spectral Mixture Analysis to Vessel Monitoring Using Airborne Hyperspectral Data.” Remote Sensing 12 (18): 2968. https://doi.org/10.3390/rs12182968."
  },
  {
    "objectID": "Week_1.html#what-is-remote-sensing",
    "href": "Week_1.html#what-is-remote-sensing",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.1 What is remote sensing?",
    "text": "1.1 What is remote sensing?\nRemote sensing is the process of acquiring of information from a distance. This week’s learning diary entry will provide a basic overview of the many uses of remote sensing as well as some of its possible applications."
  },
  {
    "objectID": "Week_1.html#sensors",
    "href": "Week_1.html#sensors",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Sensors",
    "text": "1.2 Sensors\nIn terms of sensors used to acquire remote data, there are two types: passive and active.\n\n1.2.1 Passive\nPassive sensors pick up energy either emitted by an object or reflected from it; these types of sensors do not emit any energy themselves. The majority of passive systems utilised in remote sensing applications function within the visible, infrared, thermal infrared, and microwave segments of the electromagnetic spectrum. Examples include: human eye, camera, satellite sensor, etc.\n\n\n\n(“Passive Sensors” 2024).\n\n\n\n\n1.2.2 Active\nActive sensors emit a burst of energy and analyse alterations in the returning signal. The majority of active sensors operate within the microwave segment of the electromagnetic spectrum, enabling them to penetrate the atmosphere in various conditions, e.g. clouds. Examples include: LiDAR, X-Ray, etc.\n\n\n\n(“Active Sensors” 2024)."
  },
  {
    "objectID": "Week_1.html#electromagnetic-waves",
    "href": "Week_1.html#electromagnetic-waves",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Electromagnetic waves",
    "text": "1.3 Electromagnetic waves\nElectromagnetic radiation (EMR) comprises waves generated by the electromagnetic field, propagating through space while carrying both momentum and radiant energy of the electromagnetic nature.\nEMR goes through various alterations before prior hitting the sensor, e.g. the EMR from the sun might be scattered by particles in the atmosphere, it also might be absorbed by the Earth’s surface, etc.\nAn important term to understand is wavelength - it refers to the distance between corresponding points of two consecutive waves. Various radiation types vary in wavelength and occupy different segments of the electromagnetic spectrum. Sensors are capable of capturing these diverse wavelengths, thereby offering varying information types and operating within distinct limitations."
  },
  {
    "objectID": "Week_1.html#scattering",
    "href": "Week_1.html#scattering",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.4 Scattering",
    "text": "1.4 Scattering\nScattering happens when particles or large gas molecules within the atmosphere interact with electromagnetic radiation, causing it to deviate from its initial trajectory.\nThere are three types of atmospheric scattering:\n\nRayleigh = particles are very small compared to the wavelength of the radiation.\nMie = particles are just about the same size.\nNonselective = particles are much larger.\n\n\n\n\n(Dharaiya 2022)."
  },
  {
    "objectID": "Week_1.html#resolutions",
    "href": "Week_1.html#resolutions",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.5 Resolutions",
    "text": "1.5 Resolutions\nRemotely sensed data has four resolutions:\n\nSpatial = dimensions of individual pixels in a digital image and the corresponding area on Earth’s surface that each pixel represents.\nSpectral = sensor’s capacity to distinguish more precise wavelengths.\nTemporal = duration required for a satellite to complete one orbit and return to the same observation area.\nRadiometric = level of detail contained within each pixel, denoted by the number of bits used to represent the recorded energy."
  },
  {
    "objectID": "Week_1.html#applications",
    "href": "Week_1.html#applications",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.6 Applications",
    "text": "1.6 Applications\nSatellite remote sensing is extensively used across numerous domains, including agriculture, land use mapping and monitoring, conservation and research, disaster management, climate monitoring, urban planning, weather forecasting, forest mapping, water management, mining, etc.\nA very interesting application of using remotely sensed data in aquatic wildlife conservation is DHI Group’s teams in Malaysia and Denmark that employ remote sensing techniques to map and monitor Mangrove, Coral, and Submerged Aquatic Vegetation (MCSAV) via their web-based interactive platform called MCSAV. Through this technology, DHI endeavors to offer access to earth observation-derived maps in Malaysia, bridging the information gap and enhancing the planning, management, and monitoring of coastal and marine ecosystems. This initiative includes the two marine parks in Sabah state, aiming to safeguard ecosystems and promote biodiversity restoration. Please refer to the video below for more information on the initiative:\n\nAnother fascinating area where remotely sensed data is offering solutions to a multitude of problems is urban planning. Challenges such as urban sprawl and growth monitoring, heat island effects and microclimate analysis, disaster preparedness and response facilitation (amongst many others), all become much more manageable with satellite data.\nUtilising Land Surface Temperature (LST) data based on Ecosystem Spaceborne Thermal Radiometer Experiment on Space Station observations from 2018 to 2021, Yin et al. discovered “persistent negative correlations between daytime land surface temperature (LST) and median household income across the Los Angeles metropolitan area” (Yin et al. 2023). These findings are crucial given the projection of more frequent and intense heat waves in the future as it becomes imperative to implement fair mitigation strategies."
  },
  {
    "objectID": "Week_1.html#reflections",
    "href": "Week_1.html#reflections",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.7 Reflections",
    "text": "1.7 Reflections\n\n\n\n\n“Active Sensors.” 2024. NASA Earth Data: Open Access for Open Science. https://www.earthdata.nasa.gov/learn/backgrounders/active-sensors.\n\n\nDharaiya, Aditya. 2022. “Remote Sensing: An Overview with Fundamentals and Applications.” Prithivya 2 (1): 38–52.\n\n\n“Passive Sensors.” 2024. NASA Earth Data: Open Access for Open Science. https://www.earthdata.nasa.gov/learn/backgrounders/passive-sensors#:~:text=Passive%20sensors%20detect%20energy%20emitted,portions%20of%20the%20electromagnetic%20spectrum.\n\n\nYin, Yi, Liyin He, Paul O. Wennberg, and Christian Frankenberg. 2023. “Unequal Exposure to Heatwaves in Los Angeles: Impact of Uneven Green Spaces.” Science Advances 9 (17): eade8501. https://doi.org/10.1126/sciadv.ade8501."
  },
  {
    "objectID": "Week_2.html#xarigan-presentation-lidar-sensor",
    "href": "Week_2.html#xarigan-presentation-lidar-sensor",
    "title": "2  Xarigan",
    "section": "2.1 Xarigan presentation: LiDAR sensor",
    "text": "2.1 Xarigan presentation: LiDAR sensor\nLearning diary entry for week 2 will consist of a Xarigan presentation on the LiDAR sensor. Please find the summary, applications and reflections sections in the presentation below."
  },
  {
    "objectID": "Week_3.html#summary",
    "href": "Week_3.html#summary",
    "title": "3  Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nRemotely sensed images sometimes may be flawed (due to the atmosphere, terrain, sensor, etc.) - there are various types of corrections that can be done and four of them - geometric, atmospheric, orthorectification/topographic, and radiometric calibration - will be discussed below.\n\n3.1.1 Geometric correction\n\nIdentify Ground Control Points (GPS) to align identifiable points within the image with those in a reference dataset.\nDerive geometric transformation coefficients by modeling the coordinates.\nNext, we plot these points and aim to minimize the Root Mean Square Error (RMSE) (the model with the lowest RMSE will fit best), with Jensen defining a target RMSE value of 0.5.\nRe-sampling the final raster is a must, methods are the following:\n\nNearest Neighbour\nLinear\nCubic\nCubic spine\n\n\n\n\n\nSchematic Depiction of Geometric Correction (“Image Processing” n.d.).\n\n\n\n\n3.1.2 Atmospheric correction\nAtmospheric correction is necessary when data concerns biophysical parameters as well as when using spectral signatures over time and space.\n\n3.1.2.1 Atmospheric correction types\n\nRelative (to something):\n\nDark object subtraction (DOS) or histogram adjustment: scanning each band to identify the darkest value and then subtracting it from every pixel.\nPseudo-invariant Features (PIFs): assuming brightness pixels linearly relates to a base image - regression per band - modifying the image according to the regression outcome and applying this principle to the rest of the pixels.\n\nAbsolute:\n\nConverting digital brightness values into scaled surface reflectance - allows for comparison of these scaled surface reflectance values globally and is achieved through atmospheric radiative transfer models.\nRequirements: an atmospheric model, local atmospheric visibility, image altitude.\n\nEmpirical line correction:\n\nGoing in situ and using a field spectrometer.\n\n\n\n\n\nA true color composite (bands 4-3-2) of Sentinel-2B image with top-of-atmosphere (TOA) reflectance (left) and surface reflectance after atmospheric correction (right) with Land Surface Reflectance Code (LaSRC) (“Atmospheric Correction” n.d.).\n\n\n\n\n\n3.1.3 Orthorectification/topographic correction\nOrthorectification is the process of correcting image distortions caused by terrain variations, ensuring accurate geometric representation of Earth’s surface in remote sensing imagery i.e., as if viewed from nadir. It requires sensor geometry and an elevation model.\nFormulas to do this:\n\nCosine correction\nMinnaert correction\nStatistical empirical correction\nC correction\n\n\n\n\n(Setyawan n.d.).\n\n\n\n\n3.1.4 Radiometric calibration\nSensors capture image brightness and distribute it as a Digital Number (DN), enabling efficient storage, albeit lacking units. Radiometric calibration is essentially the process if transforming DN to spectral radiance, i.e. the amount of light within a band from a sensor in the field of view (FOV). The process of radiometric calibration is crucial in remote sensing to ensure accurate and consistent measurement of electromagnetic radiation, enabling reliable interpretation and analysis of remote sensing data.\n\n\n3.1.5 Disclaimer - or simply good news?\nRemote sensing products are now provided in a ‘corrected’ format, such as ‘Analysis Ready Data’ (ARD) i.e., there’s no need to apply these corrections manually, however, it is necessary to know how remotely sensed data was created."
  },
  {
    "objectID": "Week_3.html#applications",
    "href": "Week_3.html#applications",
    "title": "3  Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nThe two types of corrections I found most interesting personally were geometric and atmospheric, hence, the applications section will be dedicated to reviewing examples of research papers that have utilised them.\n\n3.2.1 Geometric correction\nKim, Son, and Kim (2018) published a study aimed to improve the accuracy of satellite data from ocean color sensors, like the Geostationary Ocean Color Imager (GOCI), by developing a new method for geometric correction. This was a critical gap in research to fill because traditional methods based on ground control points (GCPs) were ineffective in areas where shorelines were absent, presenting a challenge for accurately aligning satellite images. The researchers used a combination of shoreline matching and frequency matching techniques to correct the images. Their findings showed that this new method significantly improved the accuracy of the satellite data, demonstrating the importance of geometric correction in producing reliable and high-quality images for various applications, such as monitoring ocean health and environmental changes.\n\n\n\nGOCI slot imaging sequence (Kim, Son, and Kim 2018).\n\n\n\n\n3.2.2 Atmospheric correction\nSriwongsitanon, Surakit, and Thianpopirug (2011) carried out a study to investigate the impact of atmospheric correction and sampling point density on the precision of remote sensing in assessing lake water clarity. Field experiments were conducted at Bung Boraphet, the largest freshwater lake in Thailand, to gather data on clarity parameters and suspended sediment concentration. By applying atmospheric correction to satellite images, the authors identified substantial effects on the estimated values of water clarity parameters, the accuracy of which is crucial for assessing water quality, monitoring ecosystem health, and informing management decisions to protect freshwater resources."
  },
  {
    "objectID": "Week_3.html#reflections",
    "href": "Week_3.html#reflections",
    "title": "3  Corrections",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\n\n\n\n\n“Atmospheric Correction.” n.d. NASA. Accessed February 15, 2024. https://hls.gsfc.nasa.gov/algorithms/atmospheric-correction/.\n\n\n“Image Processing.” n.d. Science Education through Earth Observation for High Schools (SEOS). Accessed February 15, 2024. https://seos-project.eu/remotesensing/remotesensing-c05-p01.html.\n\n\nKim, Han-Gyeol, Jong-Hwan Son, and Taejung Kim. 2018. “Geometric Correction for the Geostationary Ocean Color Imager from a Combination of Shoreline Matching and Frequency Matching.” Sensors 18 (11): 3599. https://doi.org/10.3390/s18113599.\n\n\nSetyawan, Eric. n.d. “Orthorectification in a Nutshell.” Intermap. Accessed February 15, 2024. https://www.intermap.com/blog/orthorectification-in-a-nutshell.\n\n\nSriwongsitanon, Nutchanart, Kritsanat Surakit, and Sansarith Thianpopirug. 2011. “Influence of Atmospheric Correction and Number of Sampling Points on the Accuracy of Water Clarity Assessment Using Remote Sensing Application.” Journal of Hydrology 401 (3-4): 203–20. https://doi.org/10.1016/j.jhydrol.2011.02.023."
  },
  {
    "objectID": "Week_4.html#manila-using-remotely-sensed-data-to-deal-with-urban-heat-islands",
    "href": "Week_4.html#manila-using-remotely-sensed-data-to-deal-with-urban-heat-islands",
    "title": "4  Policy",
    "section": "4.1 Manila: using remotely sensed data to deal with urban heat islands",
    "text": "4.1 Manila: using remotely sensed data to deal with urban heat islands\nMy chosen city is Manila, the capital of the Philippines, located in the northern Philippines archipelago.\n\n\n\nSource: (Ang 2019)."
  },
  {
    "objectID": "Week_5.html#summary",
    "href": "Week_5.html#summary",
    "title": "5  Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThis week’s lecture provided an introduction to Google Earth Engine (GEE) - content below will aim to summarise the most important things to note about this platform and discuss its many applications.\nWhat is GEE? A cloud-based platform for analysing geospatial data and conducting large-scale remote sensing analysis using Javascript (code runs on both server (backend) and client (frontend) side).\nWhy should we use it? For one thing, it significantly decreases processing time and is openly available to anyone who wishes to use it. More specifically, though, the entire image collection does not need to be loaded repeatedly during looping.\nThings to know when using GEE:\n\nRaster = image\nCollection = several images or polygons\nImage scale = pixel resolution (set by the output not input)\nGEE transforms all data into the Mercator projection (EPSG: 3857)\nObject = vector, raster, feature, string, number\nGeometry = point/line/polygon (no attributes)\nFeature = geometry (with attributes)\nFeature collection = several features\n\nWhat can we do in GEE?\n\nGeometry operations (e.g. spatial operations) e.g., joins, filtering, zonal statistics, etc.\nMethods e.g., machine learning, classification, deep learning, etc.\nApplications/outputs e.g., online charts, scalable applications for geospatial data with GEE, etc.\n\n\n\n\nThe Earth Engine Code Editor (“Get Started with Earth Engine” 2023)."
  },
  {
    "objectID": "Week_5.html#applications",
    "href": "Week_5.html#applications",
    "title": "5  Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nAs far as I’m aware, Amani et al. (2020) have summarised all possible applications of GEE within the realm of academic research in the most concise and comprehensive way. A total of 450 journal articles, spanning 150 journals, were analysed from January 2010 to May 2020 - you can see their findings below:\n\n\n\nGEE applications (Amani et al. 2020).\n\n\n\n\n\nNumber and percentage of journal papers related to GEE applications, published in each discipline provided in the previous figure (Amani et al. 2020).\n\n\nConsidering how with each new year the news headlines are more and more populated with increasing volumes of natural disasters all across the world but especially in the most deprived regions, I found the application of GEE in disaster management most interesting and a positive development given the platform’s free-for-all access.\nI found quite a few studies that utilised GEE to estimate projected damage following various natural disasters. An article by Kulinan et al. (2024) used the platform to address the limitations of existing techniques for assessing wildfire damage by proposing an integrated machine learning approach using Sentinel-2 imagery from GEE. By leveraging auto-generated training samples and object-based image analysis (OBIA), the study sought to automate and improve the accuracy of wildfire damage assessments. This research conducted in South Korea demonstrated the effectiveness of the proposed method in rapidly estimating burn severity and facilitating immediate mitigation actions during wildfires. Compared to traditional satellite imagery analysis methods, the approach utilising GEE offered greater efficiency, automation, and accuracy, reaffirming its significance in enhancing rapid disaster response and management efforts.\n\n\n\nExample of Random Forest classifier showing the advantage of object-based classification in the removal of salt-and-pepper noise in Uljin and Gangneung. (Kulinan et al. 2024).\n\n\nMoving from damage projections to recovery monitoring, a study by Ghaffarian, Rezaie Farhadabad, and Kerle (2020) assessed GEE’s suitability in tracking post-disaster recovery following Typhoon Haiyan’s impact on Leyte island, Philippines. By leveraging GEE’s remote sensing data and computing capabilities, the study developed a method to generate damage and recovery maps using Landsat satellite imagery. Results showed significant recovery over three years, emphasising GEE’s potential for monitoring large-scale recovery processes. However, limitations in analysing complex urban areas due to the lack of high-resolution data were noted. Nevertheless, even considering certain limitations, GEE offers relatively rapid and cost-effective tools for assessing recovery, surpassing traditional satellite imagery analysis methods."
  },
  {
    "objectID": "Week_5.html#reflections",
    "href": "Week_5.html#reflections",
    "title": "5  Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\n“Get Started with Earth Engine.” 2023. Google Earth Engine. 2023. https://developers.google.com/earth-engine/guides/getstarted.\n\n\nGhaffarian, Saman, Ali Rezaie Farhadabad, and Norman Kerle. 2020. “Post-Disaster Recovery Monitoring with Google Earth Engine.” Applied Sciences 10 (13): 4574. https://doi.org/10.3390/app10134574.\n\n\nKulinan, Almo Senja, Younghyun Cho, Minsoo Park, and Seunghee Park. 2024. “Rapid Wildfire Damage Estimation Using Integrated Object-Based Classification with Auto-Generated Training Samples from Sentinel-2 Imagery on Google Earth Engine.” International Journal of Applied Earth Observation and Geoinformation 126 (February): 103628. https://doi.org/10.1016/j.jag.2023.103628."
  },
  {
    "objectID": "Week_6.html#summary",
    "href": "Week_6.html#summary",
    "title": "6  Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nThis week’s content focused on classification in remote sensing and - since there’s a sequel to come - more specifically, the ways of using classified data and types of classification.\nA quick recap on research areas where remotely sensed data could be used:\n\nUrban expansion\nAir pollution and Land Use Land Cover (LULC)\nUrban green spaces\nMonitoring forests and illegal logging\nForest fires\n\nAll of the studies in fields mentioned above would utilise Earth Observation (EO) data to extract land cover information - how exactly can we do that?\nExpert systems = a computational system designed to tackle problems that typically necessitate human intelligence by leveraging human knowledge. But how can human knowledge be replicated by a computer?\nHere’s where machine learning comes in.\nMachine learning = a branch of artificial intelligence focused on developing algorithms that enable computers to learn from and make predictions or decisions based on data.\n\n6.1.1 Types of classification used in remote sensing\n\n6.1.1.1 Classification and regression trees (CART) classification\n\nClassification trees categorise data into multiple discrete categories, each having distinct values.\n\n\n\n\nExample of a classification tree (Li 2019).\n\n\nWhen constructing a decision tree, the terminal nodes may consist of a blend of different categories, resulting in impurity - quantify this using Gini Impurity. The node with the lowest impurity is placed at the top of the tree as the root for initiating decision making. Subsequently, the Gini impurity is utilised at each branch to further partition the nodes. As splitting is no longer necessary, the nodes transition into leaves, and the output is determined by the majority vote.\n\nRegression trees predict continuous dependent variable.\n\n\nA few considerations:\n\nHow do we choose where the breaks would be in our data? Do we consider residuals?\nWhat happens if linear regression is not suitable for the data?\n\nPartition the data into sections based on specific thresholds (nodes) and compute the sum of squared residuals;\nExamine the sum of squared residuals (SSR) across various thresholds, identify the one with the lowest SSR to establish as the root of the tree. Repeat this process.\n\n\nNotes on overfitting:\n\nIdeally, our best model would have low bias (i.e., small difference between predicted value and true value) and low variance (i.e., low variability of model for a given point).\nTo prevent overfitting, we can either prune weakest links or limit how trees grow.\n\n\n\n6.1.1.2 Random Forests\nRandom forests is a multitude of classification decision trees. They are created by constructing multiple decision trees from bootstrapped samples of the training data and using random subsets of features for each tree’s node splitting.\n\n\n\nExample of random forest classification (Shafi 2023).\n\n\n\n\n\n6.1.2 Image classification\nConvert each pixel in the image into one of predetermined categorical classifications, then proceed with either a supervised or unsupervised classification approach.\n\n6.1.2.1 Unsupervised\nUsually referred to as clustering or k-means: distribute points randomly or uniformly throughout the spectral feature space or the first principal component analysis (PCA), and repeat until reaching a specified number of iterations or when no pixels are left unallocated.\n\n\n6.1.2.2 Supervised\nRecent studies utilise machine learning or expert systems (such as Support Vector Machines (i.e., linear binary classifier), Neural Networks (NN)) and spectral mixture analysis."
  },
  {
    "objectID": "Week_6.html#applications",
    "href": "Week_6.html#applications",
    "title": "6  Classification I",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nWhile there’s countless ways in which the previously mentioned models could be applied to, I have chosen to review a few studies that specifically used Random Forests classifier for remotely sensed image classification, mostly because I have had the least interaction with this classifier in lectures before and was keen to see how researchers have been using it recently.\nGuo et al. (2011) published a study that aimed to assess the importance of input features in urban mapping by utilising a multi-source framework combining aerial lidar (multi-echo and full waveform) and multispectral image data. It employed the Random Forests algorithm as a classifier due to its efficiency with large datasets and ability to provide measures of feature importance for each class. By using the margin theory as a confidence measure, the study aimed to confirm the relevance of input features for urban classification. The quantitative results affirmed the significance of integrating optical multispectral and lidar data, highlighting the importance of full-waveform lidar features for distinguishing between building and vegetation areas. This research is crucial for urban scene recognition and urban policy development, as it provides insights into effective methods for utilising advanced technologies in urban mapping and classification.\nAlmost poetic - the article by Linhui, Weipeng, and Huihui (2021) aimed to improve the accuracy of classifying forest types in the Laoshan construction area of the Maoershan Forest Farm, Heilongjiang Province, using remote sensing images and a Random Forests classifier. It addressed the challenge of low accuracy in extracting forest type information from high-resolution images and the lack of effective identification methods. By employing a multi-source dataset including GF-2 remote sensing images, aerial RGB images, and forest inventory data, the study utilised multiscale segmentation and feature space construction to establish an object-oriented random forest (RF) scheme. Comparative experiments with the support vector machine (SVM) classifier demonstrated that the RF scheme achieved higher overall accuracy and kappa coefficient, indicating its effectiveness in improving the classification accuracy of forest types. This research contributes to advancing land cover classification and land use/land cover (LULC) monitoring by providing a more accurate and reliable method for identifying and monitoring forest resources."
  },
  {
    "objectID": "Week_6.html#reflections",
    "href": "Week_6.html#reflections",
    "title": "6  Classification I",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\n\n\n\n\nGuo, Li, Nesrine Chehata, Clément Mallet, and Samia Boukir. 2011. “Relevance of Airborne Lidar and Multispectral Image Data for Urban Scene Classification Using Random Forests.” ISPRS Journal of Photogrammetry and Remote Sensing 66 (1): 56–66. https://doi.org/10.1016/j.isprsjprs.2010.08.007.\n\n\nLi, Lorraine. 2019. “Classification and Regression Analysis with Decision Trees.” 2019. https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054.\n\n\nLinhui, Li, Jing Weipeng, and Wang Huihui. 2021. “Extracting the Forest Type From Remote Sensing Images by Random Forest.” IEEE Sensors Journal 21 (16): 17447–54. https://doi.org/10.1109/JSEN.2020.3045501.\n\n\nShafi, Adam. 2023. “Random Forest Classification with Scikit-Learn.” DataCamp. 2023. https://www.datacamp.com/tutorial/random-forests-classifier-python."
  },
  {
    "objectID": "Week_8.html#summary",
    "href": "Week_8.html#summary",
    "title": "8  Temperature",
    "section": "8.1 Summary",
    "text": "8.1 Summary"
  },
  {
    "objectID": "Week_8.html#applications",
    "href": "Week_8.html#applications",
    "title": "8  Temperature",
    "section": "8.2 Applications",
    "text": "8.2 Applications"
  },
  {
    "objectID": "Week_8.html#reflections",
    "href": "Week_8.html#reflections",
    "title": "8  Temperature",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "“Active Sensors.” 2024. NASA Earth Data: Open\nAccess for Open Science. https://www.earthdata.nasa.gov/learn/backgrounders/active-sensors.\n\n\n“Atmospheric Correction.” n.d. NASA. Accessed February 15,\n2024. https://hls.gsfc.nasa.gov/algorithms/atmospheric-correction/.\n\n\nDharaiya, Aditya. 2022. “Remote Sensing: An Overview\nwith Fundamentals and Applications.” Prithivya 2 (1):\n38–52.\n\n\n“Image Processing.” n.d. Science Education\nthrough Earth Observation for High Schools (SEOS). Accessed February 15,\n2024. https://seos-project.eu/remotesensing/remotesensing-c05-p01.html.\n\n\nKim, Han-Gyeol, Jong-Hwan Son, and Taejung Kim. 2018. “Geometric\nCorrection for the Geostationary Ocean Color\nImager from a Combination of Shoreline\nMatching and Frequency Matching.”\nSensors 18 (11): 3599. https://doi.org/10.3390/s18113599.\n\n\n“Passive Sensors.” 2024. NASA Earth Data: Open\nAccess for Open Science. https://www.earthdata.nasa.gov/learn/backgrounders/passive-sensors#:~:text=Passive%20sensors%20detect%20energy%20emitted,portions%20of%20the%20electromagnetic%20spectrum.\n\n\nSetyawan, Eric. n.d. “Orthorectification in a\nNutshell.” Intermap. Accessed February 15, 2024. https://www.intermap.com/blog/orthorectification-in-a-nutshell.\n\n\nSriwongsitanon, Nutchanart, Kritsanat Surakit, and Sansarith\nThianpopirug. 2011. “Influence of Atmospheric Correction and\nNumber of Sampling Points on the Accuracy of Water Clarity Assessment\nUsing Remote Sensing Application.” Journal of Hydrology\n401 (3-4): 203–20. https://doi.org/10.1016/j.jhydrol.2011.02.023.\n\n\nYin, Yi, Liyin He, Paul O. Wennberg, and Christian Frankenberg. 2023.\n“Unequal Exposure to Heatwaves in Los Angeles:\nImpact of Uneven Green Spaces.” Science\nAdvances 9 (17): eade8501. https://doi.org/10.1126/sciadv.ade8501."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "About\nHi everyone!\nMy name is Aida - I was born and raised in Vilnius, Lithuania, before moving to London to do my undergraduate degree in BSc Economics and Geography at University College London. I am currently pursuing a postgraduate degree in MSc Social and Geographic Data Science (in UCL Geography department once again!).\nOutside of my studies, I am a massive endurance sports enthusiast. Last year I summitted Mount Kilimanjaro (5895m) and this year I decided to take on my first ultramarathon (58km) in May (with a 75km one to follow later this year). Some of my biggest dreams are to tackle a full distance Ironman triathlon and as many races as possible from the UTMB series, however, one step at a time for now!"
  },
  {
    "objectID": "Week_4.html",
    "href": "Week_4.html",
    "title": "4  Policy",
    "section": "",
    "text": "5 Manila: using remotely sensed data to deal with urban heat islands"
  },
  {
    "objectID": "Week_4.html#summary",
    "href": "Week_4.html#summary",
    "title": "4  Policy",
    "section": "4.2 Summary",
    "text": "4.2 Summary\nBefore even starting to discuss Manila specifically, it must be noted that the Philippines ranks as one of the most rapidly urbanising countries in the world, with more than 60% or its population living in cities; what is more, this number is predicted to reach 84% by 2050 (“Addressing Urban Heat in Metro Manila Cities: Urban Heat in the Philippine Context” 2019). As a capital city, it is not surprising that Manila is disproportionately burdened by the extreme rates of urban sprawl and population growth the country as a whole is facing.\n\n\n\nChanges in land cover map of Metro Manila indicating urban sprawl growth over a period of 34 years (LANDSAT TM, 23 December 1972, 25 January 1989, and 12 January 2006) (Andong and Sajor 2017).\n\n\nAs urban sprawl continues and population growth surges in Metro Manila, the city grapples with a multitude of challenges like traffic congestion, air pollution, informal settlements/slums, inadequate infrastructure, flooding, waste management challenges, longer hotter season (El Niño Phenomena), and vulnerability to natural disasters, factors all of which collectively exacerbate the urban heat island (i.e.,a phenomenon wherein urban regions encounter higher temperatures compared to adjacent rural areas) problem in the metropolitan area. Urban heat islands (UHI) can lead to heightened risks of health-related issues, heightened energy usage, increased pollution levels, and compromised water quality (Zhou et al. 2018). Given the potential adverse effects on cities and their residents, it’s imperative to utilise existing resources and data to identify and measure these impacts.\nThrough the promotion of inclusivity, safety, and resilience in cities and human settlements, SDG 11, among the United Nations’ “17 Sustainable Development Goals”, underscores the significant role that cities hold in the global political agenda (WorldBank 2017). SDG Goal 11, aimed at making cities and human settlements inclusive, safe, resilient, and sustainable, directly aligns with Manila’s context by highlighting the city’s ongoing endeavors to combat urban heat island effects, enhance infrastructure resilience, and promote equitable access to green spaces, thereby fostering a more livable and sustainable urban environment for all residents.\nIn Manila, the responsibility for tackling urban heat island (UHI) issues would likely fall under the jurisdiction of multiple government agencies, including the Department of Environment and Natural Resources (DENR), the Metropolitan Manila Development Authority (MMDA), The Department of Human Settlements and Urban Development (DHSUD), and the local government units (LGUs) of the smaller areal units within Metro Manila (MM). To my knowledge, these agencies often collaborate to implement policies and initiatives aimed at mitigating UHI effects and improving urban climate resilience.\nIn a relatively recent report to the World Bank, MMDA and DHSUD have outlined their action plan detailing UHI mitigation measures proposed (“Addressing Urban Heat in Metro Manila Cities: Urban Heat in the Philippine Context” 2019). Suggested solutions rely heavily on previous successful case studies from cities like Guangzhou (i.e., as a Sponge City — Haizhu wetlands), Paris (i.e., as a Sponge and Waterproof City), and, most importantly, Singapore (i.e, Green Plan 2030, Digital Urban Climate Twin). Science-based UHI strategies through scenario building and comprehensive mapping are identified as a key takeaway, suggested transfer to the MM context includes the following:\n\nCapacity-building\n\nAcquisition/development of programs and platforms\nInstallation and rehabilitation of sensors\nWorkshops on UHI\n\nIdentification of Baseline, KPIs and Targets\n\nStakeholder consultation\n\nSurveys\nMulti-agency meetings\n\n\nGeospatial analysis\n\nMapping of vulnerability of MM cities to UHI\nMapping of existing Urban Green Spaces\n\nMonitoring and Evaluation\n\nAssessment through KPIs\nEstablishment of feedback mechanism\n\nValuation of natural resources"
  },
  {
    "objectID": "Week_4.html#applications",
    "href": "Week_4.html#applications",
    "title": "4  Policy",
    "section": "4.3 Applications",
    "text": "4.3 Applications\nWhile “Addressing Urban Heat in Metro Manila Cities: Urban Heat in the Philippine Context” (2019) does not go into detailed plans on how this action plan will be implemented in reality, there is no doubt that remotely sensed data will be crucial throughout the process, particularly during the geospatial analysis as well as monitoring and evaluation stages.\nFor mapping the vulnerability of MM cities to UHI and identifying existing urban green spaces, data from satellites and sensors such as Landsat, Sentinel-2, and MODIS can be utilised. These platforms provide multispectral and thermal imagery suitable for assessing land surface temperature variations and vegetation cover. Additionally, sensors like ASTER (Advanced Spaceborne Thermal Emission and Reflection Radiometer) and WorldView offer high-resolution imagery for detailed analysis of urban features and green spaces. Integrating data from these sources enables comprehensive geospatial analysis to support urban planning and management efforts in addressing UHI and enhancing green infrastructure."
  },
  {
    "objectID": "Week_4.html#reflections",
    "href": "Week_4.html#reflections",
    "title": "4  Policy",
    "section": "4.5 Reflections",
    "text": "4.5 Reflections\n\n\n\n\n“Addressing Urban Heat in Metro Manila Cities: Urban Heat in the Philippine Context.” 2019. World Bank. https://www.thegpsc.org/sites/gpsc/files/5._philippines.pdf.\n\n\nAndong, Rebeca Fontanilla, and Edsel Sajor. 2017. “Urban Sprawl, Public Transport, and Increasing CO2 Emissions: The Case of Metro Manila, Philippines.” Environment, Development and Sustainability 19 (1): 99–123. https://doi.org/10.1007/s10668-015-9729-8.\n\n\nAng, Raymond. 2019. “Where To Stay in Manila.” Forbes Vetted. 2019. https://www.forbes.com/sites/raymondang/2019/07/07/where-to-stay-in-manila/.\n\n\nWorldBank. 2017. “Issue Brief for SDG 11—Sustainable Cities and Communities: Make Cities and Human Settlements Inclusive, Safe, Resilient, and Sustainable.” Washington, D.C.: World Bank.\n\n\nZhou, Decheng, Jingfeng Xiao, Stefania Bonafoni, Christian Berger, Kaveh Deilami, Yuyu Zhou, Steve Frolking, Rui Yao, Zhi Qiao, and José Sobrino. 2018. “Satellite Remote Sensing of Surface Urban Heat Islands: Progress, Challenges, and Perspectives.” Remote Sensing 11 (1): 48. https://doi.org/10.3390/rs11010048."
  },
  {
    "objectID": "Week_4.html#limitations",
    "href": "Week_4.html#limitations",
    "title": "4  Policy",
    "section": "4.4 Limitations",
    "text": "4.4 Limitations\n\n\n\nSource: (“Addressing Urban Heat in Metro Manila Cities: Urban Heat in the Philippine Context” 2019)."
  }
]